{
 "cells": [
  {
   "source": [
    "# Installation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "variable-flavor",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T10:25:08.557872Z",
     "iopub.status.busy": "2021-06-10T10:25:08.557363Z",
     "iopub.status.idle": "2021-06-10T10:25:15.795106Z",
     "shell.execute_reply": "2021-06-10T10:25:15.794581Z"
    },
    "papermill": {
     "duration": 7.247556,
     "end_time": "2021-06-10T10:25:15.795242",
     "exception": false,
     "start_time": "2021-06-10T10:25:08.547686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optuna in /opt/conda/lib/python3.7/site-packages (2.7.0)\r\n",
      "Requirement already satisfied: colorlog in /opt/conda/lib/python3.7/site-packages (from optuna) (5.0.1)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from optuna) (1.19.5)\r\n",
      "Requirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from optuna) (1.5.8)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from optuna) (4.59.0)\r\n",
      "Requirement already satisfied: scipy!=1.4.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.5.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (20.9)\r\n",
      "Requirement already satisfied: cliff in /opt/conda/lib/python3.7/site-packages (from optuna) (3.7.0)\r\n",
      "Requirement already satisfied: sqlalchemy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from optuna) (1.4.3)\r\n",
      "Requirement already satisfied: cmaes>=0.8.2 in /opt/conda/lib/python3.7/site-packages (from optuna) (0.8.2)\r\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->optuna) (2.4.7)\r\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna) (3.4.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy>=1.1.0->optuna) (1.0.0)\r\n",
      "Requirement already satisfied: python-editor>=0.3 in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (1.0.4)\r\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (1.1.4)\r\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from alembic->optuna) (2.8.1)\r\n",
      "Requirement already satisfied: PyYAML>=3.12 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (5.3.1)\r\n",
      "Requirement already satisfied: stevedore>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (3.3.0)\r\n",
      "Requirement already satisfied: PrettyTable>=0.7.2 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (2.1.0)\r\n",
      "Requirement already satisfied: cmd2>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (1.5.0)\r\n",
      "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from cliff->optuna) (5.5.1)\r\n",
      "Requirement already satisfied: attrs>=16.3.0 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (20.3.0)\r\n",
      "Requirement already satisfied: pyperclip>=1.6 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\r\n",
      "Requirement already satisfied: colorama>=0.3.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.4.4)\r\n",
      "Requirement already satisfied: wcwidth>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\r\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.7.4.3)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.4.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->optuna) (1.1.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil->alembic->optuna) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "double-authentication",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T10:25:15.819652Z",
     "iopub.status.busy": "2021-06-10T10:25:15.816680Z",
     "iopub.status.idle": "2021-06-10T10:25:21.308253Z",
     "shell.execute_reply": "2021-06-10T10:25:21.306671Z"
    },
    "papermill": {
     "duration": 5.503891,
     "end_time": "2021-06-10T10:25:21.308413",
     "exception": false,
     "start_time": "2021-06-10T10:25:15.804522",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: main.py [-h] [--cell {RNN,LSTM,GRU}] [--hidden INT] [--nlayers INT]\r\n",
      "               [--future INT] [--bi] [--attn] [--features STR [STR ...]]\r\n",
      "               [--seqlen INT] [--bs INT] [--split FLOAT] [--shuffle]\r\n",
      "               [--loss STR] [--lr FLOAT] [--epochs FLOAT] [--drop FLOAT]\r\n",
      "               [--seed INT] [--cache STR] [--save]\r\n",
      "               datafile\r\n",
      "\r\n",
      "Crypto Regressor.\r\n",
      "\r\n",
      "positional arguments:\r\n",
      "  datafile              data file (expected csv).\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --cell {RNN,LSTM,GRU}\r\n",
      "                        recurrent cell type, one of ['RNN', 'LSTM', 'GRU']\r\n",
      "  --hidden INT          number of hidden units for RNN encoder\r\n",
      "  --nlayers INT         number of layers of the RNN encoder\r\n",
      "  --future INT          number of outputs (i.e. how many steps in the future\r\n",
      "                        to predict)\r\n",
      "  --bi                  use bidirectional encoder\r\n",
      "  --attn                use attention\r\n",
      "  --features STR [STR ...]\r\n",
      "                        features to use (default ['close']), can be one (or\r\n",
      "                        many) of ['open', 'high', 'low', 'volume', 'ma',\r\n",
      "                        'ema', 'rsi', 'premium', 'funding', 'hist']\r\n",
      "  --seqlen INT          sequence length\r\n",
      "  --bs INT              batch size\r\n",
      "  --split FLOAT         training split (default 0.95 train).\r\n",
      "  --shuffle             shuffle data\r\n",
      "  --loss STR            loss function, must be one of ['MSE', 'L1']\r\n",
      "  --lr FLOAT            initial learning rate\r\n",
      "  --epochs FLOAT        max epochs\r\n",
      "  --drop FLOAT          dropout rate\r\n",
      "  --seed INT            seed\r\n",
      "  --cache STR           cache directory to save models\r\n",
      "  --save                save training history\r\n"
     ]
    }
   ],
   "source": [
    "!python 'main.py' --help"
   ]
  },
  {
   "source": [
    "!python 'optuner.py' --help"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "published-sherman",
   "metadata": {
    "papermill": {
     "duration": 0.016212,
     "end_time": "2021-06-10T10:25:21.338249",
     "exception": false,
     "start_time": "2021-06-10T10:25:21.322037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Architectural experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "binary-grill",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T10:25:21.380513Z",
     "iopub.status.busy": "2021-06-10T10:25:21.379695Z",
     "iopub.status.idle": "2021-06-10T11:23:27.010884Z",
     "shell.execute_reply": "2021-06-10T11:23:27.011415Z"
    },
    "papermill": {
     "duration": 3485.65759,
     "end_time": "2021-06-10T11:23:27.011591",
     "exception": false,
     "start_time": "2021-06-10T10:25:21.354001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features:  close\r\n",
      "\u001b[32m[I 2021-06-10 10:25:23,880]\u001b[0m A new study created in memory with name: Optuner CryptoRegressor\u001b[0m\r\n",
      "Model has a total of 791,809 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial0.\r\n",
      "Epoch: [001/30]   Time: 3.619s   Loss: 1.111e-01   Acc: 2.52%   Loss(val): 1.38e-03   Acc(val): 96.83%\r\n",
      "Epoch: [002/30]   Time: 3.434s   Loss: 8.821e-04   Acc: 84.54%   Loss(val): 1.35e-03   Acc(val): 96.61%\r\n",
      "Epoch: [003/30]   Time: 3.428s   Loss: 8.423e-04   Acc: 82.10%   Loss(val): 1.46e-03   Acc(val): 96.34%\r\n",
      "Epoch: [004/30]   Time: 3.387s   Loss: 7.741e-04   Acc: 83.91%   Loss(val): 1.46e-03   Acc(val): 96.31%\r\n",
      "Epoch: [005/30]   Time: 3.406s   Loss: 7.397e-04   Acc: 83.78%   Loss(val): 2.27e-03   Acc(val): 95.07%\r\n",
      "Epoch: [006/30]   Time: 3.451s   Loss: 6.943e-04   Acc: 84.98%   Loss(val): 2.23e-03   Acc(val): 95.09%\r\n",
      "Epoch: [007/30]   Time: 3.460s   Loss: 6.578e-04   Acc: 84.90%   Loss(val): 1.65e-03   Acc(val): 95.92%\r\n",
      "Epoch: [008/30]   Time: 3.492s   Loss: 6.175e-04   Acc: 84.05%   Loss(val): 1.86e-03   Acc(val): 95.60%\r\n",
      "Epoch: [009/30]   Time: 3.427s   Loss: 5.871e-04   Acc: 83.50%   Loss(val): 2.03e-03   Acc(val): 95.32%\r\n",
      "Epoch: [010/30]   Time: 3.520s   Loss: 5.673e-04   Acc: 83.15%   Loss(val): 2.51e-03   Acc(val): 94.68%\r\n",
      "Epoch: [011/30]   Time: 3.406s   Loss: 5.460e-04   Acc: 81.95%   Loss(val): 2.81e-03   Acc(val): 94.28%\r\n",
      "Epoch: [012/30]   Time: 3.402s   Loss: 5.448e-04   Acc: 86.50%   Loss(val): 1.63e-03   Acc(val): 95.89%\r\n",
      "Epoch: [013/30]   Time: 3.452s   Loss: 5.225e-04   Acc: 84.55%   Loss(val): 2.09e-03   Acc(val): 95.20%\r\n",
      "Epoch: [014/30]   Time: 3.457s   Loss: 5.039e-04   Acc: 84.88%   Loss(val): 2.44e-03   Acc(val): 94.71%\r\n",
      "Epoch: [015/30]   Time: 3.499s   Loss: 4.850e-04   Acc: 86.43%   Loss(val): 1.17e-03   Acc(val): 96.65%\r\n",
      "Epoch: [016/30]   Time: 3.431s   Loss: 4.766e-04   Acc: 84.55%   Loss(val): 1.05e-03   Acc(val): 96.88%\r\n",
      "Epoch: [017/30]   Time: 3.413s   Loss: 4.890e-04   Acc: 85.08%   Loss(val): 1.45e-03   Acc(val): 96.13%\r\n",
      "Epoch: [018/30]   Time: 3.404s   Loss: 4.461e-04   Acc: 84.32%   Loss(val): 1.21e-03   Acc(val): 96.55%\r\n",
      "Epoch: [019/30]   Time: 3.397s   Loss: 4.271e-04   Acc: 83.90%   Loss(val): 9.01e-04   Acc(val): 97.20%\r\n",
      "Epoch: [020/30]   Time: 3.532s   Loss: 4.373e-04   Acc: 86.09%   Loss(val): 1.40e-03   Acc(val): 96.19%\r\n",
      "Epoch: [021/30]   Time: 3.477s   Loss: 4.160e-04   Acc: 84.44%   Loss(val): 1.47e-03   Acc(val): 96.05%\r\n",
      "Epoch: [022/30]   Time: 3.496s   Loss: 4.221e-04   Acc: 85.36%   Loss(val): 1.36e-03   Acc(val): 96.24%\r\n",
      "Epoch: [023/30]   Time: 3.404s   Loss: 3.993e-04   Acc: 84.83%   Loss(val): 9.78e-04   Acc(val): 96.97%\r\n",
      "Epoch: [024/30]   Time: 3.428s   Loss: 4.169e-04   Acc: 86.08%   Loss(val): 9.66e-04   Acc(val): 96.99%\r\n",
      "Epoch: [025/30]   Time: 3.412s   Loss: 4.147e-04   Acc: 84.66%   Loss(val): 9.41e-04   Acc(val): 97.03%\r\n",
      "Epoch: [026/30]   Time: 3.390s   Loss: 3.864e-04   Acc: 86.54%   Loss(val): 1.08e-03   Acc(val): 96.72%\r\n",
      "Epoch: [027/30]   Time: 3.449s   Loss: 3.933e-04   Acc: 85.37%   Loss(val): 1.01e-03   Acc(val): 96.86%\r\n",
      "Epoch: [028/30]   Time: 3.474s   Loss: 3.825e-04   Acc: 86.17%   Loss(val): 1.14e-03   Acc(val): 96.58%\r\n",
      "Epoch: [029/30]   Time: 3.472s   Loss: 3.689e-04   Acc: 84.90%   Loss(val): 7.12e-04   Acc(val): 97.57%\r\n",
      "Epoch: [030/30]   Time: 3.470s   Loss: 3.545e-04   Acc: 87.75%   Loss(val): 6.38e-04   Acc(val): 97.81%\r\n",
      "\t  * Test: Loss 0.001\tAcc: 97.803%\r\n",
      "\u001b[32m[I 2021-06-10 10:27:14,868]\u001b[0m Trial 0 finished with value: 0.0006249126126931515 and parameters: {'hidden': 256, 'nlayers': 2}. Best is trial 0 with value: 0.0006249126126931515.\u001b[0m\r\n",
      "Model has a total of 5,257,729 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial1.\r\n",
      "Epoch: [001/30]   Time: 11.252s   Loss: 6.531e-02   Acc: 3.60%   Loss(val): 1.30e-03   Acc(val): 96.80%\r\n",
      "Epoch: [002/30]   Time: 11.251s   Loss: 8.874e-04   Acc: 82.64%   Loss(val): 1.20e-03   Acc(val): 96.94%\r\n",
      "Epoch: [003/30]   Time: 11.293s   Loss: 8.083e-04   Acc: 82.88%   Loss(val): 1.14e-03   Acc(val): 96.99%\r\n",
      "Epoch: [004/30]   Time: 11.250s   Loss: 7.362e-04   Acc: 82.60%   Loss(val): 1.34e-03   Acc(val): 96.70%\r\n",
      "Epoch: [005/30]   Time: 11.252s   Loss: 7.352e-04   Acc: 81.62%   Loss(val): 1.15e-03   Acc(val): 97.00%\r\n",
      "Epoch: [006/30]   Time: 11.282s   Loss: 6.750e-04   Acc: 82.53%   Loss(val): 1.66e-03   Acc(val): 95.88%\r\n",
      "Epoch: [007/30]   Time: 11.256s   Loss: 6.310e-04   Acc: 81.42%   Loss(val): 1.30e-03   Acc(val): 96.69%\r\n",
      "Epoch: [008/30]   Time: 11.253s   Loss: 6.022e-04   Acc: 83.22%   Loss(val): 9.84e-04   Acc(val): 97.27%\r\n",
      "Epoch: [009/30]   Time: 11.271s   Loss: 5.970e-04   Acc: 83.74%   Loss(val): 1.30e-03   Acc(val): 96.64%\r\n",
      "Epoch: [010/30]   Time: 11.262s   Loss: 6.236e-04   Acc: 84.58%   Loss(val): 1.20e-03   Acc(val): 96.81%\r\n",
      "Epoch: [011/30]   Time: 11.260s   Loss: 5.510e-04   Acc: 84.01%   Loss(val): 1.63e-03   Acc(val): 96.00%\r\n",
      "Epoch: [012/30]   Time: 11.267s   Loss: 5.700e-04   Acc: 83.70%   Loss(val): 8.28e-04   Acc(val): 97.49%\r\n",
      "Epoch: [013/30]   Time: 11.261s   Loss: 5.210e-04   Acc: 85.32%   Loss(val): 8.31e-04   Acc(val): 97.40%\r\n",
      "Epoch: [014/30]   Time: 11.266s   Loss: 5.242e-04   Acc: 84.66%   Loss(val): 1.23e-03   Acc(val): 96.66%\r\n",
      "Epoch: [015/30]   Time: 11.264s   Loss: 5.286e-04   Acc: 84.11%   Loss(val): 9.04e-04   Acc(val): 97.14%\r\n",
      "Epoch: [016/30]   Time: 11.261s   Loss: 4.792e-04   Acc: 85.19%   Loss(val): 1.04e-03   Acc(val): 97.01%\r\n",
      "Epoch: [017/30]   Time: 11.264s   Loss: 4.732e-04   Acc: 85.00%   Loss(val): 7.45e-04   Acc(val): 97.62%\r\n",
      "Epoch: [018/30]   Time: 11.261s   Loss: 4.927e-04   Acc: 86.13%   Loss(val): 8.42e-04   Acc(val): 97.25%\r\n",
      "Epoch: [019/30]   Time: 11.270s   Loss: 4.533e-04   Acc: 84.60%   Loss(val): 1.45e-03   Acc(val): 96.05%\r\n",
      "Epoch: [020/30]   Time: 11.263s   Loss: 4.815e-04   Acc: 84.96%   Loss(val): 7.37e-04   Acc(val): 97.51%\r\n",
      "Epoch: [021/30]   Time: 11.256s   Loss: 4.464e-04   Acc: 85.10%   Loss(val): 7.99e-04   Acc(val): 97.33%\r\n",
      "Epoch: [022/30]   Time: 11.258s   Loss: 4.963e-04   Acc: 85.58%   Loss(val): 6.59e-04   Acc(val): 97.71%\r\n",
      "Epoch: [023/30]   Time: 11.264s   Loss: 4.304e-04   Acc: 86.44%   Loss(val): 6.55e-04   Acc(val): 97.71%\r\n",
      "Epoch: [024/30]   Time: 11.254s   Loss: 4.504e-04   Acc: 85.47%   Loss(val): 6.32e-04   Acc(val): 97.76%\r\n",
      "Epoch: [025/30]   Time: 11.251s   Loss: 4.309e-04   Acc: 85.25%   Loss(val): 6.21e-04   Acc(val): 97.80%\r\n",
      "Epoch: [026/30]   Time: 11.254s   Loss: 4.237e-04   Acc: 85.53%   Loss(val): 6.63e-04   Acc(val): 97.65%\r\n",
      "Epoch: [027/30]   Time: 11.254s   Loss: 4.067e-04   Acc: 86.75%   Loss(val): 1.69e-03   Acc(val): 95.61%\r\n",
      "Epoch: [028/30]   Time: 11.253s   Loss: 4.370e-04   Acc: 87.27%   Loss(val): 6.50e-04   Acc(val): 97.72%\r\n",
      "Epoch: [029/30]   Time: 11.265s   Loss: 4.016e-04   Acc: 86.31%   Loss(val): 6.05e-04   Acc(val): 97.79%\r\n",
      "Epoch: [030/30]   Time: 11.253s   Loss: 3.981e-04   Acc: 85.74%   Loss(val): 6.25e-04   Acc(val): 97.73%\r\n",
      "\t  * Test: Loss 0.001\tAcc: 97.729%\r\n",
      "\u001b[32m[I 2021-06-10 10:32:56,483]\u001b[0m Trial 1 finished with value: 0.000627508637990104 and parameters: {'hidden': 512, 'nlayers': 3}. Best is trial 0 with value: 0.0006249126126931515.\u001b[0m\r\n",
      "Model has a total of 199,297 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial2.\r\n",
      "Epoch: [001/30]   Time: 2.147s   Loss: 2.076e-01   Acc: -41.34%   Loss(val): 4.27e-01   Acc(val): 20.24%\r\n",
      "Epoch: [002/30]   Time: 2.142s   Loss: 8.869e-03   Acc: 72.62%   Loss(val): 1.59e-03   Acc(val): 96.33%\r\n",
      "Epoch: [003/30]   Time: 2.218s   Loss: 1.394e-03   Acc: 81.61%   Loss(val): 1.29e-03   Acc(val): 96.82%\r\n",
      "Epoch: [004/30]   Time: 2.136s   Loss: 1.280e-03   Acc: 81.68%   Loss(val): 1.21e-03   Acc(val): 96.95%\r\n",
      "Epoch: [005/30]   Time: 2.127s   Loss: 1.197e-03   Acc: 82.50%   Loss(val): 1.16e-03   Acc(val): 97.04%\r\n",
      "Epoch: [006/30]   Time: 2.147s   Loss: 1.069e-03   Acc: 83.74%   Loss(val): 1.12e-03   Acc(val): 97.09%\r\n",
      "Epoch: [007/30]   Time: 2.170s   Loss: 9.789e-04   Acc: 83.55%   Loss(val): 1.19e-03   Acc(val): 96.97%\r\n",
      "Epoch: [008/30]   Time: 2.449s   Loss: 9.039e-04   Acc: 83.15%   Loss(val): 1.37e-03   Acc(val): 96.60%\r\n",
      "Epoch: [009/30]   Time: 2.178s   Loss: 8.356e-04   Acc: 83.98%   Loss(val): 1.09e-03   Acc(val): 97.10%\r\n",
      "Epoch: [010/30]   Time: 2.172s   Loss: 8.167e-04   Acc: 84.70%   Loss(val): 1.11e-03   Acc(val): 97.06%\r\n",
      "Epoch: [011/30]   Time: 2.139s   Loss: 7.774e-04   Acc: 83.68%   Loss(val): 1.15e-03   Acc(val): 96.95%\r\n",
      "Epoch: [012/30]   Time: 2.134s   Loss: 7.114e-04   Acc: 83.06%   Loss(val): 1.22e-03   Acc(val): 96.80%\r\n",
      "Epoch: [013/30]   Time: 2.165s   Loss: 6.922e-04   Acc: 84.99%   Loss(val): 1.11e-03   Acc(val): 97.02%\r\n",
      "Epoch: [014/30]   Time: 2.174s   Loss: 6.921e-04   Acc: 82.78%   Loss(val): 1.27e-03   Acc(val): 96.73%\r\n",
      "Epoch: [015/30]   Time: 2.149s   Loss: 6.655e-04   Acc: 85.43%   Loss(val): 1.15e-03   Acc(val): 96.93%\r\n",
      "Epoch: [016/30]   Time: 2.152s   Loss: 6.398e-04   Acc: 84.49%   Loss(val): 1.28e-03   Acc(val): 96.72%\r\n",
      "Epoch: [017/30]   Time: 2.179s   Loss: 6.236e-04   Acc: 85.05%   Loss(val): 1.11e-03   Acc(val): 97.00%\r\n",
      "Epoch: [018/30]   Time: 2.177s   Loss: 5.955e-04   Acc: 83.74%   Loss(val): 1.24e-03   Acc(val): 96.77%\r\n",
      "Epoch: [019/30]   Time: 2.132s   Loss: 5.919e-04   Acc: 85.23%   Loss(val): 1.25e-03   Acc(val): 96.75%\r\n",
      "Epoch: [020/30]   Time: 2.144s   Loss: 5.831e-04   Acc: 83.77%   Loss(val): 1.27e-03   Acc(val): 96.71%\r\n",
      "Epoch: [021/30]   Time: 2.177s   Loss: 5.752e-04   Acc: 84.01%   Loss(val): 1.03e-03   Acc(val): 97.12%\r\n",
      "Epoch: [022/30]   Time: 2.142s   Loss: 5.443e-04   Acc: 82.98%   Loss(val): 1.09e-03   Acc(val): 97.01%\r\n",
      "Epoch: [023/30]   Time: 2.215s   Loss: 5.412e-04   Acc: 83.21%   Loss(val): 1.02e-03   Acc(val): 97.13%\r\n",
      "Epoch: [024/30]   Time: 2.402s   Loss: 5.319e-04   Acc: 84.92%   Loss(val): 1.21e-03   Acc(val): 96.79%\r\n",
      "Epoch: [025/30]   Time: 2.133s   Loss: 5.187e-04   Acc: 84.81%   Loss(val): 1.60e-03   Acc(val): 96.11%\r\n",
      "Epoch: [026/30]   Time: 2.132s   Loss: 5.212e-04   Acc: 83.95%   Loss(val): 1.18e-03   Acc(val): 96.83%\r\n",
      "Epoch: [027/30]   Time: 2.144s   Loss: 5.113e-04   Acc: 84.28%   Loss(val): 9.49e-04   Acc(val): 97.25%\r\n",
      "Epoch: [028/30]   Time: 2.222s   Loss: 4.866e-04   Acc: 84.47%   Loss(val): 1.23e-03   Acc(val): 96.71%\r\n",
      "Epoch: [029/30]   Time: 2.137s   Loss: 4.937e-04   Acc: 84.50%   Loss(val): 9.33e-04   Acc(val): 97.27%\r\n",
      "Epoch: [030/30]   Time: 2.126s   Loss: 4.796e-04   Acc: 85.14%   Loss(val): 9.89e-04   Acc(val): 97.15%\r\n",
      "\t  * Test: Loss 0.001\tAcc: 97.103%\r\n",
      "\u001b[32m[I 2021-06-10 10:34:02,693]\u001b[0m Trial 2 finished with value: 0.0010274367450620048 and parameters: {'hidden': 128, 'nlayers': 2}. Best is trial 0 with value: 0.0006249126126931515.\u001b[0m\r\n",
      "Model has a total of 150,337 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial3.\r\n",
      "Epoch: [001/30]   Time: 7.151s   Loss: 2.157e-01   Acc: -98.81%   Loss(val): 9.58e-01   Acc(val): -19.81%\r\n",
      "Epoch: [002/30]   Time: 7.283s   Loss: 3.015e-02   Acc: 46.81%   Loss(val): 1.77e-02   Acc(val): 86.01%\r\n",
      "Epoch: [003/30]   Time: 7.398s   Loss: 3.791e-03   Acc: 70.74%   Loss(val): 3.42e-03   Acc(val): 94.54%\r\n",
      "Epoch: [004/30]   Time: 7.171s   Loss: 2.801e-03   Acc: 76.46%   Loss(val): 2.61e-03   Acc(val): 95.31%\r\n",
      "Epoch: [005/30]   Time: 7.271s   Loss: 2.317e-03   Acc: 75.00%   Loss(val): 2.96e-03   Acc(val): 94.99%\r\n",
      "Epoch: [006/30]   Time: 7.179s   Loss: 2.088e-03   Acc: 78.68%   Loss(val): 3.14e-03   Acc(val): 94.83%\r\n",
      "Epoch: [007/30]   Time: 7.339s   Loss: 1.908e-03   Acc: 72.26%   Loss(val): 4.28e-03   Acc(val): 93.77%\r\n",
      "Epoch: [008/30]   Time: 7.235s   Loss: 1.692e-03   Acc: 75.76%   Loss(val): 3.29e-03   Acc(val): 94.69%\r\n",
      "Epoch: [009/30]   Time: 7.195s   Loss: 1.642e-03   Acc: 71.87%   Loss(val): 3.62e-03   Acc(val): 94.35%\r\n",
      "Epoch: [010/30]   Time: 7.197s   Loss: 1.547e-03   Acc: 72.39%   Loss(val): 2.94e-03   Acc(val): 94.97%\r\n",
      "Epoch: [011/30]   Time: 7.280s   Loss: 1.509e-03   Acc: 76.95%   Loss(val): 3.06e-03   Acc(val): 94.90%\r\n",
      "Epoch: [012/30]   Time: 7.390s   Loss: 1.439e-03   Acc: 78.14%   Loss(val): 3.30e-03   Acc(val): 94.68%\r\n",
      "Epoch: [013/30]   Time: 7.201s   Loss: 1.372e-03   Acc: 76.49%   Loss(val): 3.42e-03   Acc(val): 94.60%\r\n",
      "Epoch: [014/30]   Time: 7.285s   Loss: 1.356e-03   Acc: 76.23%   Loss(val): 3.35e-03   Acc(val): 94.66%\r\n",
      "Epoch: [015/30]   Time: 7.197s   Loss: 1.356e-03   Acc: 73.86%   Loss(val): 3.31e-03   Acc(val): 94.69%\r\n",
      "Epoch: [016/30]   Time: 7.469s   Loss: 1.300e-03   Acc: 78.06%   Loss(val): 4.11e-03   Acc(val): 94.07%\r\n",
      "Epoch: [017/30]   Time: 7.239s   Loss: 1.222e-03   Acc: 79.04%   Loss(val): 3.44e-03   Acc(val): 94.55%\r\n",
      "Epoch: [018/30]   Time: 7.176s   Loss: 1.235e-03   Acc: 82.44%   Loss(val): 3.76e-03   Acc(val): 94.33%\r\n",
      "Epoch: [019/30]   Time: 7.290s   Loss: 1.196e-03   Acc: 78.46%   Loss(val): 3.58e-03   Acc(val): 94.49%\r\n",
      "Epoch: [020/30]   Time: 7.235s   Loss: 1.167e-03   Acc: 77.81%   Loss(val): 3.46e-03   Acc(val): 94.56%\r\n",
      "Epoch: [021/30]   Time: 7.343s   Loss: 1.164e-03   Acc: 77.53%   Loss(val): 3.83e-03   Acc(val): 94.31%\r\n",
      "Epoch: [022/30]   Time: 7.276s   Loss: 1.189e-03   Acc: 77.16%   Loss(val): 3.92e-03   Acc(val): 94.24%\r\n",
      "Epoch: [023/30]   Time: 7.216s   Loss: 1.088e-03   Acc: 74.79%   Loss(val): 3.79e-03   Acc(val): 94.35%\r\n",
      "Epoch: [024/30]   Time: 7.150s   Loss: 1.114e-03   Acc: 73.81%   Loss(val): 3.73e-03   Acc(val): 94.39%\r\n",
      "Epoch: [025/30]   Time: 7.143s   Loss: 1.079e-03   Acc: 78.47%   Loss(val): 3.88e-03   Acc(val): 94.27%\r\n",
      "Epoch: [026/30]   Time: 7.205s   Loss: 1.088e-03   Acc: 77.56%   Loss(val): 3.56e-03   Acc(val): 94.53%\r\n",
      "Epoch: [027/30]   Time: 7.171s   Loss: 1.075e-03   Acc: 80.46%   Loss(val): 3.67e-03   Acc(val): 94.45%\r\n",
      "Epoch: [028/30]   Time: 7.290s   Loss: 1.034e-03   Acc: 80.55%   Loss(val): 3.35e-03   Acc(val): 94.60%\r\n",
      "Epoch: [029/30]   Time: 7.164s   Loss: 1.032e-03   Acc: 79.27%   Loss(val): 3.81e-03   Acc(val): 94.35%\r\n",
      "Epoch: [030/30]   Time: 6.940s   Loss: 1.030e-03   Acc: 78.59%   Loss(val): 3.23e-03   Acc(val): 94.79%\r\n",
      "\t  * Test: Loss 0.003\tAcc: 94.733%\r\n",
      "\u001b[32m[I 2021-06-10 10:37:40,746]\u001b[0m Trial 3 finished with value: 0.0032469635480083525 and parameters: {'hidden': 64, 'nlayers': 5}. Best is trial 0 with value: 0.0006249126126931515.\u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial4.\r\n",
      "Epoch: [001/30]   Time: 7.165s   Loss: 6.564e-02   Acc: 26.48%   Loss(val): 1.49e-03   Acc(val): 96.68%\r\n",
      "Epoch: [002/30]   Time: 7.263s   Loss: 4.171e-04   Acc: 84.77%   Loss(val): 1.29e-03   Acc(val): 96.76%\r\n",
      "Epoch: [003/30]   Time: 7.257s   Loss: 4.073e-04   Acc: 84.86%   Loss(val): 1.28e-03   Acc(val): 96.69%\r\n",
      "Epoch: [004/30]   Time: 7.204s   Loss: 3.877e-04   Acc: 84.96%   Loss(val): 1.28e-03   Acc(val): 96.62%\r\n",
      "Epoch: [005/30]   Time: 7.309s   Loss: 3.724e-04   Acc: 85.09%   Loss(val): 1.18e-03   Acc(val): 96.78%\r\n",
      "Epoch: [006/30]   Time: 7.269s   Loss: 3.501e-04   Acc: 85.24%   Loss(val): 1.15e-03   Acc(val): 96.79%\r\n",
      "Epoch: [007/30]   Time: 7.281s   Loss: 3.290e-04   Acc: 85.41%   Loss(val): 9.72e-04   Acc(val): 97.18%\r\n",
      "Epoch: [008/30]   Time: 7.167s   Loss: 3.138e-04   Acc: 85.53%   Loss(val): 8.90e-04   Acc(val): 97.36%\r\n",
      "Epoch: [009/30]   Time: 7.258s   Loss: 3.021e-04   Acc: 85.61%   Loss(val): 8.67e-04   Acc(val): 97.36%\r\n",
      "Epoch: [010/30]   Time: 7.260s   Loss: 2.927e-04   Acc: 85.70%   Loss(val): 8.53e-04   Acc(val): 97.32%\r\n",
      "Epoch: [011/30]   Time: 7.198s   Loss: 2.833e-04   Acc: 85.80%   Loss(val): 8.34e-04   Acc(val): 97.32%\r\n",
      "Epoch: [012/30]   Time: 7.308s   Loss: 2.737e-04   Acc: 85.89%   Loss(val): 8.17e-04   Acc(val): 97.32%\r\n",
      "Epoch: [013/30]   Time: 7.255s   Loss: 2.643e-04   Acc: 86.00%   Loss(val): 8.01e-04   Acc(val): 97.33%\r\n",
      "Epoch: [014/30]   Time: 7.318s   Loss: 2.552e-04   Acc: 86.13%   Loss(val): 7.86e-04   Acc(val): 97.35%\r\n",
      "Epoch: [015/30]   Time: 7.171s   Loss: 2.462e-04   Acc: 86.29%   Loss(val): 7.71e-04   Acc(val): 97.37%\r\n",
      "Epoch: [016/30]   Time: 7.252s   Loss: 2.373e-04   Acc: 86.46%   Loss(val): 7.55e-04   Acc(val): 97.40%\r\n",
      "Epoch: [017/30]   Time: 7.269s   Loss: 2.288e-04   Acc: 86.64%   Loss(val): 7.35e-04   Acc(val): 97.44%\r\n",
      "Epoch: [018/30]   Time: 7.184s   Loss: 2.223e-04   Acc: 86.80%   Loss(val): 7.17e-04   Acc(val): 97.48%\r\n",
      "Epoch: [019/30]   Time: 7.307s   Loss: 2.182e-04   Acc: 86.95%   Loss(val): 6.94e-04   Acc(val): 97.54%\r\n",
      "Epoch: [020/30]   Time: 7.264s   Loss: 2.147e-04   Acc: 87.11%   Loss(val): 6.65e-04   Acc(val): 97.61%\r\n",
      "Epoch: [021/30]   Time: 7.281s   Loss: 2.108e-04   Acc: 87.29%   Loss(val): 6.36e-04   Acc(val): 97.68%\r\n",
      "Epoch: [022/30]   Time: 7.162s   Loss: 2.068e-04   Acc: 87.47%   Loss(val): 6.10e-04   Acc(val): 97.74%\r\n",
      "Epoch: [023/30]   Time: 7.250s   Loss: 2.028e-04   Acc: 87.65%   Loss(val): 5.89e-04   Acc(val): 97.79%\r\n",
      "Epoch: [024/30]   Time: 7.259s   Loss: 1.988e-04   Acc: 87.84%   Loss(val): 5.71e-04   Acc(val): 97.83%\r\n",
      "Epoch: [025/30]   Time: 7.186s   Loss: 1.948e-04   Acc: 88.04%   Loss(val): 5.57e-04   Acc(val): 97.86%\r\n",
      "Epoch: [026/30]   Time: 7.311s   Loss: 1.908e-04   Acc: 88.23%   Loss(val): 5.47e-04   Acc(val): 97.89%\r\n",
      "Epoch: [027/30]   Time: 7.267s   Loss: 1.868e-04   Acc: 88.42%   Loss(val): 5.40e-04   Acc(val): 97.90%\r\n",
      "Epoch: [028/30]   Time: 7.286s   Loss: 1.827e-04   Acc: 88.62%   Loss(val): 5.37e-04   Acc(val): 97.90%\r\n",
      "Epoch: [029/30]   Time: 7.171s   Loss: 1.790e-04   Acc: 88.81%   Loss(val): 5.26e-04   Acc(val): 97.92%\r\n",
      "Epoch: [030/30]   Time: 7.252s   Loss: 1.756e-04   Acc: 89.00%   Loss(val): 5.03e-04   Acc(val): 97.99%\r\n",
      "\t  * Test: Loss 0.001\tAcc: 97.984%\r\n",
      "\u001b[32m[I 2021-06-10 10:41:20,362]\u001b[0m Trial 4 finished with value: 0.0005078321955807041 and parameters: {'hidden': 1024, 'nlayers': 1}. Best is trial 4 with value: 0.0005078321955807041.\u001b[0m\r\n",
      "Model has a total of 150,337 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial5.\r\n",
      "Epoch: [001/30]   Time: 7.231s   Loss: 2.364e-01   Acc: -128.58%   Loss(val): 9.18e-01   Acc(val): -17.60%\r\n",
      "\u001b[32m[I 2021-06-10 10:41:28,403]\u001b[0m Trial 5 pruned. \u001b[0m\r\n",
      "Model has a total of 595,585 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial6.\r\n",
      "Epoch: [001/30]   Time: 9.055s   Loss: 1.757e-01   Acc: -77.16%   Loss(val): 5.13e-01   Acc(val): 12.48%\r\n",
      "\u001b[32m[I 2021-06-10 10:41:38,410]\u001b[0m Trial 6 pruned. \u001b[0m\r\n",
      "Model has a total of 9,460,225 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial7.\r\n",
      "Epoch: [001/30]   Time: 20.974s   Loss: 7.322e-02   Acc: 17.40%   Loss(val): 2.23e-03   Acc(val): 95.56%\r\n",
      "\u001b[32m[I 2021-06-10 10:42:00,388]\u001b[0m Trial 7 pruned. \u001b[0m\r\n",
      "Model has a total of 595,585 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial8.\r\n",
      "Epoch: [001/30]   Time: 9.053s   Loss: 1.689e-01   Acc: -70.57%   Loss(val): 6.65e-01   Acc(val): 0.15%\r\n",
      "\u001b[32m[I 2021-06-10 10:42:10,272]\u001b[0m Trial 8 pruned. \u001b[0m\r\n",
      "Model has a total of 83,777 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial9.\r\n",
      "Epoch: [001/30]   Time: 3.417s   Loss: 2.625e-01   Acc: -51.18%   Loss(val): 7.95e-01   Acc(val): -9.45%\r\n",
      "\u001b[32m[I 2021-06-10 10:42:14,487]\u001b[0m Trial 9 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial10.\r\n",
      "Epoch: [001/30]   Time: 7.200s   Loss: 7.366e-02   Acc: 17.20%   Loss(val): 1.39e-03   Acc(val): 96.77%\r\n",
      "Epoch: [002/30]   Time: 7.258s   Loss: 3.961e-04   Acc: 84.91%   Loss(val): 1.32e-03   Acc(val): 96.84%\r\n",
      "Epoch: [003/30]   Time: 7.260s   Loss: 3.808e-04   Acc: 85.20%   Loss(val): 1.10e-03   Acc(val): 97.16%\r\n",
      "Epoch: [004/30]   Time: 7.179s   Loss: 3.663e-04   Acc: 85.16%   Loss(val): 1.04e-03   Acc(val): 97.22%\r\n",
      "Epoch: [005/30]   Time: 7.275s   Loss: 3.534e-04   Acc: 85.15%   Loss(val): 9.95e-04   Acc(val): 97.26%\r\n",
      "Epoch: [006/30]   Time: 7.258s   Loss: 3.428e-04   Acc: 85.16%   Loss(val): 9.58e-04   Acc(val): 97.28%\r\n",
      "Epoch: [007/30]   Time: 7.209s   Loss: 3.323e-04   Acc: 85.18%   Loss(val): 9.40e-04   Acc(val): 97.25%\r\n",
      "Epoch: [008/30]   Time: 7.209s   Loss: 3.222e-04   Acc: 85.17%   Loss(val): 8.91e-04   Acc(val): 97.34%\r\n",
      "Epoch: [009/30]   Time: 7.257s   Loss: 3.097e-04   Acc: 85.20%   Loss(val): 8.40e-04   Acc(val): 97.46%\r\n",
      "Epoch: [010/30]   Time: 7.270s   Loss: 2.974e-04   Acc: 85.22%   Loss(val): 8.15e-04   Acc(val): 97.53%\r\n",
      "Epoch: [011/30]   Time: 7.184s   Loss: 2.840e-04   Acc: 85.30%   Loss(val): 8.09e-04   Acc(val): 97.54%\r\n",
      "Epoch: [012/30]   Time: 7.276s   Loss: 2.735e-04   Acc: 85.44%   Loss(val): 7.69e-04   Acc(val): 97.60%\r\n",
      "Epoch: [013/30]   Time: 7.253s   Loss: 2.648e-04   Acc: 85.59%   Loss(val): 7.28e-04   Acc(val): 97.65%\r\n",
      "Epoch: [014/30]   Time: 7.222s   Loss: 2.566e-04   Acc: 85.78%   Loss(val): 6.94e-04   Acc(val): 97.69%\r\n",
      "Epoch: [015/30]   Time: 7.203s   Loss: 2.487e-04   Acc: 85.93%   Loss(val): 6.73e-04   Acc(val): 97.70%\r\n",
      "Epoch: [016/30]   Time: 7.250s   Loss: 2.419e-04   Acc: 86.06%   Loss(val): 6.55e-04   Acc(val): 97.72%\r\n",
      "Epoch: [017/30]   Time: 7.243s   Loss: 2.363e-04   Acc: 86.17%   Loss(val): 6.41e-04   Acc(val): 97.73%\r\n",
      "Epoch: [018/30]   Time: 7.185s   Loss: 2.314e-04   Acc: 86.30%   Loss(val): 6.28e-04   Acc(val): 97.75%\r\n",
      "Epoch: [019/30]   Time: 7.276s   Loss: 2.270e-04   Acc: 86.43%   Loss(val): 6.15e-04   Acc(val): 97.77%\r\n",
      "Epoch: [020/30]   Time: 7.256s   Loss: 2.229e-04   Acc: 86.56%   Loss(val): 6.03e-04   Acc(val): 97.79%\r\n",
      "Epoch: [021/30]   Time: 7.221s   Loss: 2.191e-04   Acc: 86.70%   Loss(val): 5.91e-04   Acc(val): 97.81%\r\n",
      "Epoch: [022/30]   Time: 7.198s   Loss: 2.155e-04   Acc: 86.84%   Loss(val): 5.81e-04   Acc(val): 97.83%\r\n",
      "Epoch: [023/30]   Time: 7.263s   Loss: 2.119e-04   Acc: 86.98%   Loss(val): 5.70e-04   Acc(val): 97.85%\r\n",
      "Epoch: [024/30]   Time: 7.265s   Loss: 2.083e-04   Acc: 87.14%   Loss(val): 5.58e-04   Acc(val): 97.87%\r\n",
      "Epoch: [025/30]   Time: 7.174s   Loss: 2.047e-04   Acc: 87.30%   Loss(val): 5.46e-04   Acc(val): 97.90%\r\n",
      "Epoch: [026/30]   Time: 7.270s   Loss: 2.013e-04   Acc: 87.48%   Loss(val): 5.35e-04   Acc(val): 97.93%\r\n",
      "Epoch: [027/30]   Time: 7.262s   Loss: 1.979e-04   Acc: 87.66%   Loss(val): 5.23e-04   Acc(val): 97.95%\r\n",
      "Epoch: [028/30]   Time: 7.220s   Loss: 1.946e-04   Acc: 87.84%   Loss(val): 5.12e-04   Acc(val): 97.98%\r\n",
      "Epoch: [029/30]   Time: 7.212s   Loss: 1.914e-04   Acc: 88.04%   Loss(val): 5.00e-04   Acc(val): 98.01%\r\n",
      "Epoch: [030/30]   Time: 7.246s   Loss: 1.882e-04   Acc: 88.23%   Loss(val): 4.89e-04   Acc(val): 98.04%\r\n",
      "\t  * Test: Loss 0.000\tAcc: 98.038%\r\n",
      "\u001b[32m[I 2021-06-10 10:45:54,761]\u001b[0m Trial 10 finished with value: 0.0004906899121124297 and parameters: {'hidden': 1024, 'nlayers': 1}. Best is trial 10 with value: 0.0004906899121124297.\u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial11.\r\n",
      "Epoch: [001/30]   Time: 7.197s   Loss: 6.390e-02   Acc: 30.05%   Loss(val): 1.52e-03   Acc(val): 96.27%\r\n",
      "\u001b[32m[I 2021-06-10 10:46:02,836]\u001b[0m Trial 11 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial12.\r\n",
      "Epoch: [001/30]   Time: 7.296s   Loss: 6.856e-02   Acc: 29.38%   Loss(val): 1.24e-03   Acc(val): 96.99%\r\n",
      "Epoch: [002/30]   Time: 7.247s   Loss: 4.240e-04   Acc: 85.62%   Loss(val): 1.22e-03   Acc(val): 97.00%\r\n",
      "Epoch: [003/30]   Time: 7.216s   Loss: 3.966e-04   Acc: 86.03%   Loss(val): 1.10e-03   Acc(val): 97.15%\r\n",
      "Epoch: [004/30]   Time: 7.228s   Loss: 3.814e-04   Acc: 86.30%   Loss(val): 1.04e-03   Acc(val): 97.22%\r\n",
      "Epoch: [005/30]   Time: 7.266s   Loss: 3.676e-04   Acc: 86.45%   Loss(val): 9.85e-04   Acc(val): 97.26%\r\n",
      "Epoch: [006/30]   Time: 7.277s   Loss: 3.428e-04   Acc: 86.67%   Loss(val): 9.39e-04   Acc(val): 97.31%\r\n",
      "Epoch: [007/30]   Time: 7.207s   Loss: 3.287e-04   Acc: 86.73%   Loss(val): 8.95e-04   Acc(val): 97.38%\r\n",
      "Epoch: [008/30]   Time: 7.282s   Loss: 3.188e-04   Acc: 86.80%   Loss(val): 8.56e-04   Acc(val): 97.44%\r\n",
      "Epoch: [009/30]   Time: 7.263s   Loss: 3.078e-04   Acc: 86.89%   Loss(val): 8.20e-04   Acc(val): 97.49%\r\n",
      "Epoch: [010/30]   Time: 7.217s   Loss: 2.979e-04   Acc: 86.96%   Loss(val): 7.88e-04   Acc(val): 97.53%\r\n",
      "Epoch: [011/30]   Time: 7.231s   Loss: 2.881e-04   Acc: 87.05%   Loss(val): 7.60e-04   Acc(val): 97.56%\r\n",
      "Epoch: [012/30]   Time: 7.259s   Loss: 2.793e-04   Acc: 87.15%   Loss(val): 7.36e-04   Acc(val): 97.58%\r\n",
      "Epoch: [013/30]   Time: 7.277s   Loss: 2.710e-04   Acc: 87.27%   Loss(val): 7.15e-04   Acc(val): 97.60%\r\n",
      "Epoch: [014/30]   Time: 7.210s   Loss: 2.634e-04   Acc: 87.41%   Loss(val): 6.97e-04   Acc(val): 97.62%\r\n",
      "Epoch: [015/30]   Time: 7.274s   Loss: 2.563e-04   Acc: 87.56%   Loss(val): 6.80e-04   Acc(val): 97.64%\r\n",
      "Epoch: [016/30]   Time: 7.258s   Loss: 2.499e-04   Acc: 87.70%   Loss(val): 6.65e-04   Acc(val): 97.67%\r\n",
      "Epoch: [017/30]   Time: 7.220s   Loss: 2.441e-04   Acc: 87.86%   Loss(val): 6.51e-04   Acc(val): 97.69%\r\n",
      "Epoch: [018/30]   Time: 7.224s   Loss: 2.387e-04   Acc: 88.02%   Loss(val): 6.37e-04   Acc(val): 97.72%\r\n",
      "Epoch: [019/30]   Time: 7.255s   Loss: 2.337e-04   Acc: 88.18%   Loss(val): 6.24e-04   Acc(val): 97.74%\r\n",
      "Epoch: [020/30]   Time: 7.278s   Loss: 2.290e-04   Acc: 88.34%   Loss(val): 6.11e-04   Acc(val): 97.77%\r\n",
      "Epoch: [021/30]   Time: 7.206s   Loss: 2.246e-04   Acc: 88.49%   Loss(val): 5.99e-04   Acc(val): 97.79%\r\n",
      "Epoch: [022/30]   Time: 7.276s   Loss: 2.204e-04   Acc: 88.65%   Loss(val): 5.87e-04   Acc(val): 97.82%\r\n",
      "Epoch: [023/30]   Time: 7.267s   Loss: 2.164e-04   Acc: 88.82%   Loss(val): 5.75e-04   Acc(val): 97.84%\r\n",
      "Epoch: [024/30]   Time: 7.222s   Loss: 2.125e-04   Acc: 88.99%   Loss(val): 5.64e-04   Acc(val): 97.86%\r\n",
      "Epoch: [025/30]   Time: 7.228s   Loss: 2.088e-04   Acc: 89.17%   Loss(val): 5.54e-04   Acc(val): 97.89%\r\n",
      "Epoch: [026/30]   Time: 7.258s   Loss: 2.051e-04   Acc: 89.35%   Loss(val): 5.44e-04   Acc(val): 97.91%\r\n",
      "Epoch: [027/30]   Time: 7.285s   Loss: 2.016e-04   Acc: 89.54%   Loss(val): 5.36e-04   Acc(val): 97.92%\r\n",
      "Epoch: [028/30]   Time: 7.206s   Loss: 1.982e-04   Acc: 89.73%   Loss(val): 5.27e-04   Acc(val): 97.94%\r\n",
      "Epoch: [029/30]   Time: 7.277s   Loss: 1.948e-04   Acc: 89.92%   Loss(val): 5.20e-04   Acc(val): 97.95%\r\n",
      "Epoch: [030/30]   Time: 7.241s   Loss: 1.915e-04   Acc: 90.11%   Loss(val): 5.13e-04   Acc(val): 97.96%\r\n",
      "\t  * Test: Loss 0.001\tAcc: 97.998%\r\n",
      "\u001b[32m[I 2021-06-10 10:49:43,322]\u001b[0m Trial 12 finished with value: 0.0005007083054806571 and parameters: {'hidden': 1024, 'nlayers': 1}. Best is trial 10 with value: 0.0004906899121124297.\u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial13.\r\n",
      "Epoch: [001/30]   Time: 7.214s   Loss: 6.365e-02   Acc: 31.81%   Loss(val): 2.16e-03   Acc(val): 95.27%\r\n",
      "\u001b[32m[I 2021-06-10 10:49:51,453]\u001b[0m Trial 13 pruned. \u001b[0m\r\n",
      "Model has a total of 29,398,017 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial14.\r\n",
      "Epoch: [001/30]   Time: 38.611s   Loss: 4.631e-02   Acc: 48.95%   Loss(val): 1.61e-03   Acc(val): 96.31%\r\n",
      "\u001b[32m[I 2021-06-10 10:50:31,554]\u001b[0m Trial 14 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial15.\r\n",
      "Epoch: [001/30]   Time: 7.287s   Loss: 6.295e-02   Acc: 17.30%   Loss(val): 1.30e-03   Acc(val): 96.93%\r\n",
      "Epoch: [002/30]   Time: 7.273s   Loss: 3.803e-04   Acc: 85.84%   Loss(val): 1.16e-03   Acc(val): 97.11%\r\n",
      "Epoch: [003/30]   Time: 7.232s   Loss: 3.777e-04   Acc: 85.90%   Loss(val): 1.08e-03   Acc(val): 97.16%\r\n",
      "Epoch: [004/30]   Time: 7.229s   Loss: 3.626e-04   Acc: 86.14%   Loss(val): 1.10e-03   Acc(val): 96.99%\r\n",
      "Epoch: [005/30]   Time: 7.274s   Loss: 3.518e-04   Acc: 86.28%   Loss(val): 1.15e-03   Acc(val): 96.82%\r\n",
      "Epoch: [006/30]   Time: 7.306s   Loss: 3.456e-04   Acc: 86.33%   Loss(val): 1.18e-03   Acc(val): 96.70%\r\n",
      "Epoch: [007/30]   Time: 7.199s   Loss: 3.361e-04   Acc: 86.44%   Loss(val): 1.16e-03   Acc(val): 96.71%\r\n",
      "Epoch: [008/30]   Time: 7.283s   Loss: 3.289e-04   Acc: 86.51%   Loss(val): 1.17e-03   Acc(val): 96.63%\r\n",
      "\u001b[32m[I 2021-06-10 10:51:31,404]\u001b[0m Trial 15 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial16.\r\n",
      "Epoch: [001/30]   Time: 7.295s   Loss: 6.448e-02   Acc: 1.66%   Loss(val): 1.30e-03   Acc(val): 96.93%\r\n",
      "Epoch: [002/30]   Time: 7.223s   Loss: 4.047e-04   Acc: 86.13%   Loss(val): 1.38e-03   Acc(val): 96.74%\r\n",
      "Epoch: [003/30]   Time: 7.235s   Loss: 3.809e-04   Acc: 86.56%   Loss(val): 1.17e-03   Acc(val): 97.07%\r\n",
      "Epoch: [004/30]   Time: 7.263s   Loss: 3.697e-04   Acc: 86.53%   Loss(val): 1.11e-03   Acc(val): 97.14%\r\n",
      "Epoch: [005/30]   Time: 7.317s   Loss: 3.528e-04   Acc: 86.50%   Loss(val): 9.94e-04   Acc(val): 97.31%\r\n",
      "Epoch: [006/30]   Time: 7.216s   Loss: 3.417e-04   Acc: 86.42%   Loss(val): 9.65e-04   Acc(val): 97.33%\r\n",
      "Epoch: [007/30]   Time: 7.283s   Loss: 3.211e-04   Acc: 86.54%   Loss(val): 8.87e-04   Acc(val): 97.44%\r\n",
      "Epoch: [008/30]   Time: 7.275s   Loss: 3.034e-04   Acc: 86.68%   Loss(val): 8.33e-04   Acc(val): 97.48%\r\n",
      "Epoch: [009/30]   Time: 7.226s   Loss: 2.879e-04   Acc: 86.84%   Loss(val): 8.60e-04   Acc(val): 97.31%\r\n",
      "Epoch: [010/30]   Time: 7.242s   Loss: 2.778e-04   Acc: 86.97%   Loss(val): 9.43e-04   Acc(val): 97.05%\r\n",
      "Epoch: [011/30]   Time: 7.277s   Loss: 2.716e-04   Acc: 87.09%   Loss(val): 1.03e-03   Acc(val): 96.82%\r\n",
      "Epoch: [012/30]   Time: 7.312s   Loss: 2.652e-04   Acc: 87.25%   Loss(val): 1.09e-03   Acc(val): 96.68%\r\n",
      "\u001b[32m[I 2021-06-10 10:52:59,886]\u001b[0m Trial 16 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 265,473 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial17.\r\n",
      "Epoch: [001/30]   Time: 1.471s   Loss: 1.630e-01   Acc: -6.85%   Loss(val): 2.27e-01   Acc(val): 41.87%\r\n",
      "\u001b[32m[I 2021-06-10 10:53:02,156]\u001b[0m Trial 17 pruned. \u001b[0m\r\n",
      "Model has a total of 29,398,017 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial18.\r\n",
      "Epoch: [001/30]   Time: 38.623s   Loss: 5.288e-02   Acc: 26.24%   Loss(val): 1.80e-03   Acc(val): 95.99%\r\n",
      "\u001b[32m[I 2021-06-10 10:53:42,416]\u001b[0m Trial 18 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial19.\r\n",
      "Epoch: [001/30]   Time: 7.224s   Loss: 6.179e-02   Acc: 39.54%   Loss(val): 1.27e-03   Acc(val): 96.97%\r\n",
      "Epoch: [002/30]   Time: 7.250s   Loss: 3.991e-04   Acc: 85.62%   Loss(val): 1.24e-03   Acc(val): 96.80%\r\n",
      "Epoch: [003/30]   Time: 7.297s   Loss: 3.973e-04   Acc: 85.75%   Loss(val): 1.36e-03   Acc(val): 96.50%\r\n",
      "Epoch: [004/30]   Time: 7.203s   Loss: 3.718e-04   Acc: 86.05%   Loss(val): 1.26e-03   Acc(val): 96.64%\r\n",
      "Epoch: [005/30]   Time: 7.343s   Loss: 3.559e-04   Acc: 86.11%   Loss(val): 1.17e-03   Acc(val): 96.78%\r\n",
      "\u001b[32m[I 2021-06-10 10:54:19,917]\u001b[0m Trial 19 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 265,473 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial20.\r\n",
      "Epoch: [001/30]   Time: 1.469s   Loss: 1.321e-01   Acc: -36.24%   Loss(val): 4.29e-02   Acc(val): 75.43%\r\n",
      "\u001b[32m[I 2021-06-10 10:54:22,228]\u001b[0m Trial 20 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial21.\r\n",
      "Epoch: [001/30]   Time: 7.225s   Loss: 6.223e-02   Acc: 11.08%   Loss(val): 1.71e-03   Acc(val): 96.27%\r\n",
      "\u001b[32m[I 2021-06-10 10:54:30,349]\u001b[0m Trial 21 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial22.\r\n",
      "Epoch: [001/30]   Time: 7.215s   Loss: 7.349e-02   Acc: 15.57%   Loss(val): 1.31e-03   Acc(val): 96.73%\r\n",
      "Epoch: [002/30]   Time: 7.264s   Loss: 4.118e-04   Acc: 85.52%   Loss(val): 1.47e-03   Acc(val): 96.31%\r\n",
      "Epoch: [003/30]   Time: 7.299s   Loss: 3.944e-04   Acc: 85.64%   Loss(val): 2.00e-03   Acc(val): 95.42%\r\n",
      "\u001b[32m[I 2021-06-10 10:54:53,252]\u001b[0m Trial 22 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial23.\r\n",
      "Epoch: [001/30]   Time: 7.192s   Loss: 6.271e-02   Acc: 10.72%   Loss(val): 2.19e-03   Acc(val): 95.53%\r\n",
      "\u001b[32m[I 2021-06-10 10:55:01,367]\u001b[0m Trial 23 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial24.\r\n",
      "Epoch: [001/30]   Time: 7.277s   Loss: 6.847e-02   Acc: 48.86%   Loss(val): 2.08e-03   Acc(val): 95.68%\r\n",
      "\u001b[32m[I 2021-06-10 10:55:09,528]\u001b[0m Trial 24 pruned. \u001b[0m\r\n",
      "Model has a total of 3,156,481 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial25.\r\n",
      "Epoch: [001/30]   Time: 6.073s   Loss: 6.916e-02   Acc: 21.25%   Loss(val): 1.90e-03   Acc(val): 95.76%\r\n",
      "\u001b[32m[I 2021-06-10 10:55:16,460]\u001b[0m Trial 25 pruned. \u001b[0m\r\n",
      "Model has a total of 29,398,017 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial26.\r\n",
      "Epoch: [001/30]   Time: 38.612s   Loss: 4.434e-02   Acc: 30.90%   Loss(val): 1.57e-03   Acc(val): 96.32%\r\n",
      "\u001b[32m[I 2021-06-10 10:55:56,605]\u001b[0m Trial 26 pruned. \u001b[0m\r\n",
      "Model has a total of 21,001,217 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial27.\r\n",
      "Epoch: [001/30]   Time: 27.259s   Loss: 4.241e-02   Acc: 40.38%   Loss(val): 1.45e-03   Acc(val): 96.30%\r\n",
      "\u001b[32m[I 2021-06-10 10:56:25,208]\u001b[0m Trial 27 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial28.\r\n",
      "Epoch: [001/30]   Time: 7.270s   Loss: 6.857e-02   Acc: 16.97%   Loss(val): 1.74e-03   Acc(val): 95.84%\r\n",
      "\u001b[32m[I 2021-06-10 10:56:33,354]\u001b[0m Trial 28 pruned. \u001b[0m\r\n",
      "Model has a total of 791,809 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial29.\r\n",
      "Epoch: [001/30]   Time: 3.430s   Loss: 1.231e-01   Acc: 7.41%   Loss(val): 4.82e-03   Acc(val): 92.45%\r\n",
      "\u001b[32m[I 2021-06-10 10:56:37,651]\u001b[0m Trial 29 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial30.\r\n",
      "Epoch: [001/30]   Time: 7.282s   Loss: 6.054e-02   Acc: 20.00%   Loss(val): 1.16e-03   Acc(val): 97.04%\r\n",
      "Epoch: [002/30]   Time: 7.281s   Loss: 3.698e-04   Acc: 85.86%   Loss(val): 1.18e-03   Acc(val): 96.87%\r\n",
      "Epoch: [003/30]   Time: 7.229s   Loss: 3.611e-04   Acc: 85.79%   Loss(val): 1.06e-03   Acc(val): 97.16%\r\n",
      "Epoch: [004/30]   Time: 7.225s   Loss: 3.529e-04   Acc: 85.87%   Loss(val): 1.00e-03   Acc(val): 97.24%\r\n",
      "Epoch: [005/30]   Time: 7.256s   Loss: 3.422e-04   Acc: 85.96%   Loss(val): 1.06e-03   Acc(val): 96.98%\r\n",
      "Epoch: [006/30]   Time: 7.298s   Loss: 3.206e-04   Acc: 86.11%   Loss(val): 9.21e-04   Acc(val): 97.32%\r\n",
      "Epoch: [007/30]   Time: 7.180s   Loss: 3.058e-04   Acc: 86.20%   Loss(val): 8.68e-04   Acc(val): 97.41%\r\n",
      "Epoch: [008/30]   Time: 7.275s   Loss: 2.884e-04   Acc: 86.35%   Loss(val): 8.59e-04   Acc(val): 97.37%\r\n",
      "Epoch: [009/30]   Time: 7.279s   Loss: 2.782e-04   Acc: 86.41%   Loss(val): 8.39e-04   Acc(val): 97.36%\r\n",
      "Epoch: [010/30]   Time: 7.225s   Loss: 2.704e-04   Acc: 86.49%   Loss(val): 8.03e-04   Acc(val): 97.41%\r\n",
      "Epoch: [011/30]   Time: 7.218s   Loss: 2.601e-04   Acc: 86.61%   Loss(val): 8.08e-04   Acc(val): 97.35%\r\n",
      "Epoch: [012/30]   Time: 7.259s   Loss: 2.496e-04   Acc: 86.79%   Loss(val): 8.52e-04   Acc(val): 97.20%\r\n",
      "Epoch: [013/30]   Time: 7.281s   Loss: 2.426e-04   Acc: 86.94%   Loss(val): 9.08e-04   Acc(val): 97.04%\r\n",
      "Epoch: [014/30]   Time: 7.189s   Loss: 2.350e-04   Acc: 87.06%   Loss(val): 9.63e-04   Acc(val): 96.91%\r\n",
      "Epoch: [015/30]   Time: 7.290s   Loss: 2.277e-04   Acc: 87.21%   Loss(val): 9.90e-04   Acc(val): 96.84%\r\n",
      "Epoch: [016/30]   Time: 7.285s   Loss: 2.218e-04   Acc: 87.34%   Loss(val): 9.85e-04   Acc(val): 96.85%\r\n",
      "Epoch: [017/30]   Time: 7.227s   Loss: 2.177e-04   Acc: 87.44%   Loss(val): 9.69e-04   Acc(val): 96.88%\r\n",
      "\u001b[32m[I 2021-06-10 10:58:43,221]\u001b[0m Trial 30 pruned. \u001b[0m\r\n",
      "Model has a total of 791,809 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial31.\r\n",
      "Epoch: [001/30]   Time: 3.445s   Loss: 1.135e-01   Acc: -58.12%   Loss(val): 1.38e-03   Acc(val): 96.51%\r\n",
      "Epoch: [002/30]   Time: 3.455s   Loss: 8.723e-04   Acc: 79.78%   Loss(val): 1.12e-03   Acc(val): 97.05%\r\n",
      "Epoch: [003/30]   Time: 3.301s   Loss: 8.203e-04   Acc: 80.89%   Loss(val): 1.11e-03   Acc(val): 97.13%\r\n",
      "Epoch: [004/30]   Time: 3.334s   Loss: 7.632e-04   Acc: 79.89%   Loss(val): 1.17e-03   Acc(val): 97.02%\r\n",
      "Epoch: [005/30]   Time: 3.393s   Loss: 7.549e-04   Acc: 78.77%   Loss(val): 1.06e-03   Acc(val): 97.18%\r\n",
      "Epoch: [006/30]   Time: 3.487s   Loss: 7.059e-04   Acc: 80.32%   Loss(val): 1.03e-03   Acc(val): 97.21%\r\n",
      "Epoch: [007/30]   Time: 3.394s   Loss: 6.742e-04   Acc: 80.96%   Loss(val): 1.00e-03   Acc(val): 97.24%\r\n",
      "Epoch: [008/30]   Time: 3.441s   Loss: 6.188e-04   Acc: 81.92%   Loss(val): 9.87e-04   Acc(val): 97.28%\r\n",
      "\u001b[32m[I 2021-06-10 10:59:11,489]\u001b[0m Trial 31 pruned. \u001b[0m\r\n",
      "Model has a total of 791,809 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial32.\r\n",
      "Epoch: [001/30]   Time: 3.447s   Loss: 1.069e-01   Acc: -11.61%   Loss(val): 2.20e-03   Acc(val): 95.23%\r\n",
      "\u001b[32m[I 2021-06-10 10:59:15,735]\u001b[0m Trial 32 pruned. \u001b[0m\r\n",
      "Model has a total of 791,809 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial33.\r\n",
      "Epoch: [001/30]   Time: 3.315s   Loss: 9.987e-02   Acc: -7.61%   Loss(val): 1.22e-03   Acc(val): 96.94%\r\n",
      "Epoch: [002/30]   Time: 3.281s   Loss: 8.251e-04   Acc: 82.91%   Loss(val): 1.16e-03   Acc(val): 97.01%\r\n",
      "Epoch: [003/30]   Time: 3.406s   Loss: 8.078e-04   Acc: 83.78%   Loss(val): 1.26e-03   Acc(val): 96.69%\r\n",
      "Epoch: [004/30]   Time: 3.447s   Loss: 7.674e-04   Acc: 84.45%   Loss(val): 1.06e-03   Acc(val): 97.17%\r\n",
      "Epoch: [005/30]   Time: 3.402s   Loss: 7.305e-04   Acc: 81.88%   Loss(val): 1.21e-03   Acc(val): 96.74%\r\n",
      "Epoch: [006/30]   Time: 3.449s   Loss: 6.707e-04   Acc: 84.46%   Loss(val): 1.01e-03   Acc(val): 97.25%\r\n",
      "Epoch: [007/30]   Time: 3.418s   Loss: 6.151e-04   Acc: 83.27%   Loss(val): 1.04e-03   Acc(val): 97.19%\r\n",
      "Epoch: [008/30]   Time: 3.316s   Loss: 6.031e-04   Acc: 84.65%   Loss(val): 1.09e-03   Acc(val): 97.08%\r\n",
      "\u001b[32m[I 2021-06-10 10:59:43,797]\u001b[0m Trial 33 pruned. \u001b[0m\r\n",
      "Model has a total of 5,257,729 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial34.\r\n",
      "Epoch: [001/30]   Time: 11.193s   Loss: 6.954e-02   Acc: 28.98%   Loss(val): 1.62e-03   Acc(val): 96.32%\r\n",
      "\u001b[32m[I 2021-06-10 10:59:55,890]\u001b[0m Trial 34 pruned. \u001b[0m\r\n",
      "Model has a total of 50,497 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial35.\r\n",
      "Epoch: [001/30]   Time: 1.675s   Loss: 2.547e-01   Acc: -46.32%   Loss(val): 5.72e-01   Acc(val): 7.37%\r\n",
      "\u001b[32m[I 2021-06-10 10:59:58,383]\u001b[0m Trial 35 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 67,201 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial36.\r\n",
      "Epoch: [001/30]   Time: 0.928s   Loss: 2.723e-01   Acc: -42.97%   Loss(val): 6.32e-01   Acc(val): 2.45%\r\n",
      "\u001b[32m[I 2021-06-10 11:00:00,167]\u001b[0m Trial 36 pruned. \u001b[0m\r\n",
      "Model has a total of 791,809 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial37.\r\n",
      "Epoch: [001/30]   Time: 3.456s   Loss: 1.062e-01   Acc: -11.39%   Loss(val): 1.21e-03   Acc(val): 97.01%\r\n",
      "Epoch: [002/30]   Time: 3.448s   Loss: 8.583e-04   Acc: 82.05%   Loss(val): 1.19e-03   Acc(val): 97.04%\r\n",
      "Epoch: [003/30]   Time: 3.312s   Loss: 7.817e-04   Acc: 83.08%   Loss(val): 1.14e-03   Acc(val): 97.02%\r\n",
      "Epoch: [004/30]   Time: 3.323s   Loss: 7.497e-04   Acc: 84.44%   Loss(val): 1.10e-03   Acc(val): 97.10%\r\n",
      "Epoch: [005/30]   Time: 3.401s   Loss: 7.023e-04   Acc: 84.53%   Loss(val): 1.07e-03   Acc(val): 97.13%\r\n",
      "Epoch: [006/30]   Time: 3.476s   Loss: 6.853e-04   Acc: 84.66%   Loss(val): 1.05e-03   Acc(val): 97.14%\r\n",
      "Epoch: [007/30]   Time: 3.386s   Loss: 6.433e-04   Acc: 84.30%   Loss(val): 1.03e-03   Acc(val): 97.22%\r\n",
      "Epoch: [008/30]   Time: 3.422s   Loss: 6.198e-04   Acc: 84.15%   Loss(val): 1.01e-03   Acc(val): 97.18%\r\n",
      "\u001b[32m[I 2021-06-10 11:00:28,415]\u001b[0m Trial 37 pruned. \u001b[0m\r\n",
      "Model has a total of 595,585 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial38.\r\n",
      "Epoch: [001/30]   Time: 9.054s   Loss: 1.885e-01   Acc: -20.79%   Loss(val): 5.76e-01   Acc(val): 7.25%\r\n",
      "\u001b[32m[I 2021-06-10 11:00:38,266]\u001b[0m Trial 38 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 1,055,233 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial39.\r\n",
      "Epoch: [001/30]   Time: 3.052s   Loss: 9.392e-02   Acc: -7.97%   Loss(val): 1.36e-03   Acc(val): 96.88%\r\n",
      "Epoch: [002/30]   Time: 3.012s   Loss: 4.127e-04   Acc: 84.72%   Loss(val): 1.29e-03   Acc(val): 96.87%\r\n",
      "Epoch: [003/30]   Time: 3.051s   Loss: 4.007e-04   Acc: 84.81%   Loss(val): 1.25e-03   Acc(val): 96.89%\r\n",
      "Epoch: [004/30]   Time: 2.998s   Loss: 3.910e-04   Acc: 85.06%   Loss(val): 1.18e-03   Acc(val): 97.05%\r\n",
      "Epoch: [005/30]   Time: 3.019s   Loss: 3.799e-04   Acc: 85.38%   Loss(val): 1.13e-03   Acc(val): 97.11%\r\n",
      "Epoch: [006/30]   Time: 3.001s   Loss: 3.659e-04   Acc: 85.79%   Loss(val): 1.09e-03   Acc(val): 97.18%\r\n",
      "Epoch: [007/30]   Time: 3.018s   Loss: 3.529e-04   Acc: 85.92%   Loss(val): 1.06e-03   Acc(val): 97.23%\r\n",
      "Epoch: [008/30]   Time: 3.052s   Loss: 3.411e-04   Acc: 86.02%   Loss(val): 1.05e-03   Acc(val): 97.24%\r\n",
      "\u001b[32m[I 2021-06-10 11:01:03,352]\u001b[0m Trial 39 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 17,217 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial40.\r\n",
      "Epoch: [001/30]   Time: 0.796s   Loss: 4.514e-01   Acc: -45.33%   Loss(val): 6.81e-01   Acc(val): -1.31%\r\n",
      "\u001b[32m[I 2021-06-10 11:01:04,939]\u001b[0m Trial 40 pruned. \u001b[0m\r\n",
      "Model has a total of 5,257,729 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial41.\r\n",
      "Epoch: [001/30]   Time: 11.156s   Loss: 7.047e-02   Acc: -5.80%   Loss(val): 1.30e-03   Acc(val): 96.85%\r\n",
      "Epoch: [002/30]   Time: 11.152s   Loss: 8.701e-04   Acc: 82.97%   Loss(val): 1.21e-03   Acc(val): 96.86%\r\n",
      "Epoch: [003/30]   Time: 11.162s   Loss: 8.257e-04   Acc: 82.49%   Loss(val): 1.14e-03   Acc(val): 96.95%\r\n",
      "Epoch: [004/30]   Time: 11.147s   Loss: 7.706e-04   Acc: 82.68%   Loss(val): 1.21e-03   Acc(val): 96.72%\r\n",
      "Epoch: [005/30]   Time: 11.143s   Loss: 7.410e-04   Acc: 82.28%   Loss(val): 1.10e-03   Acc(val): 97.06%\r\n",
      "Epoch: [006/30]   Time: 11.153s   Loss: 7.196e-04   Acc: 82.12%   Loss(val): 1.02e-03   Acc(val): 97.11%\r\n",
      "Epoch: [007/30]   Time: 11.140s   Loss: 6.459e-04   Acc: 83.09%   Loss(val): 1.43e-03   Acc(val): 96.44%\r\n",
      "Epoch: [008/30]   Time: 11.137s   Loss: 6.313e-04   Acc: 81.63%   Loss(val): 9.78e-04   Acc(val): 97.14%\r\n",
      "Epoch: [009/30]   Time: 11.155s   Loss: 6.127e-04   Acc: 83.22%   Loss(val): 1.16e-03   Acc(val): 96.68%\r\n",
      "Epoch: [010/30]   Time: 11.143s   Loss: 6.065e-04   Acc: 82.00%   Loss(val): 8.81e-04   Acc(val): 97.35%\r\n",
      "Epoch: [011/30]   Time: 11.143s   Loss: 5.644e-04   Acc: 84.12%   Loss(val): 9.28e-04   Acc(val): 97.16%\r\n",
      "Epoch: [012/30]   Time: 11.145s   Loss: 5.850e-04   Acc: 84.00%   Loss(val): 1.01e-03   Acc(val): 96.93%\r\n",
      "\u001b[32m[I 2021-06-10 11:03:20,683]\u001b[0m Trial 41 pruned. \u001b[0m\r\n",
      "Model has a total of 5,257,729 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial42.\r\n",
      "Epoch: [001/30]   Time: 11.095s   Loss: 6.511e-02   Acc: 33.33%   Loss(val): 1.62e-03   Acc(val): 96.32%\r\n",
      "\u001b[32m[I 2021-06-10 11:03:32,751]\u001b[0m Trial 42 pruned. \u001b[0m\r\n",
      "Model has a total of 5,257,729 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial43.\r\n",
      "Epoch: [001/30]   Time: 11.135s   Loss: 6.645e-02   Acc: -7.04%   Loss(val): 1.25e-03   Acc(val): 96.89%\r\n",
      "Epoch: [002/30]   Time: 11.143s   Loss: 8.727e-04   Acc: 82.61%   Loss(val): 1.21e-03   Acc(val): 96.95%\r\n",
      "Epoch: [003/30]   Time: 11.137s   Loss: 8.249e-04   Acc: 83.07%   Loss(val): 1.13e-03   Acc(val): 97.03%\r\n",
      "Epoch: [004/30]   Time: 11.134s   Loss: 7.546e-04   Acc: 82.85%   Loss(val): 1.18e-03   Acc(val): 96.93%\r\n",
      "Epoch: [005/30]   Time: 11.144s   Loss: 7.131e-04   Acc: 82.20%   Loss(val): 1.22e-03   Acc(val): 96.84%\r\n",
      "Epoch: [006/30]   Time: 11.133s   Loss: 6.883e-04   Acc: 82.33%   Loss(val): 1.51e-03   Acc(val): 96.30%\r\n",
      "Epoch: [007/30]   Time: 11.138s   Loss: 6.759e-04   Acc: 82.14%   Loss(val): 1.56e-03   Acc(val): 96.18%\r\n",
      "Epoch: [008/30]   Time: 11.142s   Loss: 6.093e-04   Acc: 82.28%   Loss(val): 1.26e-03   Acc(val): 96.71%\r\n",
      "\u001b[32m[I 2021-06-10 11:05:03,399]\u001b[0m Trial 43 pruned. \u001b[0m\r\n",
      "Model has a total of 21,001,217 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial44.\r\n",
      "Epoch: [001/30]   Time: 27.169s   Loss: 3.878e-02   Acc: 41.82%   Loss(val): 1.38e-03   Acc(val): 96.63%\r\n",
      "Epoch: [002/30]   Time: 27.185s   Loss: 6.393e-04   Acc: 83.67%   Loss(val): 1.01e-03   Acc(val): 97.22%\r\n",
      "Epoch: [003/30]   Time: 27.165s   Loss: 6.188e-04   Acc: 84.22%   Loss(val): 9.77e-04   Acc(val): 97.18%\r\n",
      "Epoch: [004/30]   Time: 27.187s   Loss: 5.770e-04   Acc: 84.08%   Loss(val): 1.44e-03   Acc(val): 96.18%\r\n",
      "Epoch: [005/30]   Time: 27.170s   Loss: 5.372e-04   Acc: 83.03%   Loss(val): 8.59e-04   Acc(val): 97.37%\r\n",
      "Epoch: [006/30]   Time: 27.172s   Loss: 5.201e-04   Acc: 85.22%   Loss(val): 8.00e-04   Acc(val): 97.50%\r\n",
      "Epoch: [007/30]   Time: 27.181s   Loss: 5.078e-04   Acc: 85.43%   Loss(val): 2.67e-03   Acc(val): 94.34%\r\n",
      "Epoch: [008/30]   Time: 27.185s   Loss: 5.019e-04   Acc: 84.98%   Loss(val): 7.25e-04   Acc(val): 97.65%\r\n",
      "Epoch: [009/30]   Time: 27.170s   Loss: 4.582e-04   Acc: 85.26%   Loss(val): 8.28e-04   Acc(val): 97.27%\r\n",
      "Epoch: [010/30]   Time: 27.156s   Loss: 4.742e-04   Acc: 85.17%   Loss(val): 9.65e-04   Acc(val): 96.94%\r\n",
      "Epoch: [011/30]   Time: 27.177s   Loss: 4.700e-04   Acc: 86.13%   Loss(val): 9.31e-04   Acc(val): 96.98%\r\n",
      "Epoch: [012/30]   Time: 27.170s   Loss: 4.564e-04   Acc: 85.62%   Loss(val): 8.67e-04   Acc(val): 97.12%\r\n",
      "Epoch: [013/30]   Time: 27.167s   Loss: 4.304e-04   Acc: 85.65%   Loss(val): 1.36e-03   Acc(val): 96.14%\r\n",
      "Epoch: [014/30]   Time: 27.170s   Loss: 4.373e-04   Acc: 87.05%   Loss(val): 9.17e-04   Acc(val): 96.98%\r\n",
      "Epoch: [015/30]   Time: 27.176s   Loss: 4.330e-04   Acc: 86.78%   Loss(val): 8.71e-04   Acc(val): 97.07%\r\n",
      "Epoch: [016/30]   Time: 27.171s   Loss: 4.332e-04   Acc: 87.13%   Loss(val): 1.73e-03   Acc(val): 95.51%\r\n",
      "Epoch: [017/30]   Time: 27.166s   Loss: 4.164e-04   Acc: 86.33%   Loss(val): 6.41e-04   Acc(val): 97.65%\r\n",
      "Epoch: [018/30]   Time: 27.171s   Loss: 3.970e-04   Acc: 87.49%   Loss(val): 1.37e-03   Acc(val): 96.07%\r\n",
      "Epoch: [019/30]   Time: 27.163s   Loss: 3.733e-04   Acc: 87.59%   Loss(val): 1.57e-03   Acc(val): 95.74%\r\n",
      "Epoch: [020/30]   Time: 27.168s   Loss: 3.843e-04   Acc: 88.49%   Loss(val): 5.41e-04   Acc(val): 97.90%\r\n",
      "Epoch: [021/30]   Time: 27.168s   Loss: 4.173e-04   Acc: 87.08%   Loss(val): 1.02e-03   Acc(val): 96.70%\r\n",
      "Epoch: [022/30]   Time: 27.174s   Loss: 3.808e-04   Acc: 87.65%   Loss(val): 1.07e-03   Acc(val): 96.59%\r\n",
      "Epoch: [023/30]   Time: 27.174s   Loss: 3.408e-04   Acc: 87.47%   Loss(val): 1.22e-03   Acc(val): 96.28%\r\n",
      "Epoch: [024/30]   Time: 27.171s   Loss: 3.634e-04   Acc: 88.18%   Loss(val): 4.63e-04   Acc(val): 98.12%\r\n",
      "Epoch: [025/30]   Time: 27.192s   Loss: 3.853e-04   Acc: 87.49%   Loss(val): 5.79e-04   Acc(val): 97.73%\r\n",
      "Epoch: [026/30]   Time: 27.170s   Loss: 3.491e-04   Acc: 87.90%   Loss(val): 4.49e-04   Acc(val): 98.14%\r\n",
      "Epoch: [027/30]   Time: 27.170s   Loss: 3.407e-04   Acc: 89.10%   Loss(val): 4.48e-04   Acc(val): 98.13%\r\n",
      "Epoch: [028/30]   Time: 27.167s   Loss: 3.693e-04   Acc: 87.21%   Loss(val): 1.30e-03   Acc(val): 96.11%\r\n",
      "Epoch: [029/30]   Time: 27.170s   Loss: 3.848e-04   Acc: 87.89%   Loss(val): 1.21e-03   Acc(val): 96.26%\r\n",
      "Epoch: [030/30]   Time: 27.170s   Loss: 3.436e-04   Acc: 88.39%   Loss(val): 8.57e-04   Acc(val): 96.99%\r\n",
      "\t  * Test: Loss 0.001\tAcc: 97.009%\r\n",
      "\u001b[32m[I 2021-06-10 11:18:51,700]\u001b[0m Trial 44 finished with value: 0.0008461512697977014 and parameters: {'hidden': 1024, 'nlayers': 3}. Best is trial 10 with value: 0.0004906899121124297.\u001b[0m\r\n",
      "Model has a total of 595,585 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial45.\r\n",
      "Epoch: [001/30]   Time: 9.009s   Loss: 1.817e-01   Acc: -47.69%   Loss(val): 5.90e-01   Acc(val): 6.25%\r\n",
      "\u001b[32m[I 2021-06-10 11:19:01,558]\u001b[0m Trial 45 pruned. \u001b[0m\r\n",
      "Model has a total of 7,358,977 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial46.\r\n",
      "Epoch: [001/30]   Time: 15.990s   Loss: 7.520e-02   Acc: 9.04%   Loss(val): 2.26e-03   Acc(val): 95.48%\r\n",
      "\u001b[32m[I 2021-06-10 11:19:18,491]\u001b[0m Trial 46 pruned. \u001b[0m\r\n",
      "Model has a total of 83,777 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial47.\r\n",
      "Epoch: [001/30]   Time: 3.473s   Loss: 2.423e-01   Acc: -103.78%   Loss(val): 8.95e-01   Acc(val): -15.98%\r\n",
      "\u001b[32m[I 2021-06-10 11:19:22,754]\u001b[0m Trial 47 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial48.\r\n",
      "Epoch: [001/30]   Time: 7.288s   Loss: 6.203e-02   Acc: 34.75%   Loss(val): 1.20e-03   Acc(val): 97.05%\r\n",
      "Epoch: [002/30]   Time: 7.193s   Loss: 3.732e-04   Acc: 85.51%   Loss(val): 1.21e-03   Acc(val): 97.02%\r\n",
      "Epoch: [003/30]   Time: 7.303s   Loss: 3.614e-04   Acc: 85.39%   Loss(val): 1.11e-03   Acc(val): 97.16%\r\n",
      "Epoch: [004/30]   Time: 7.248s   Loss: 3.571e-04   Acc: 85.50%   Loss(val): 1.01e-03   Acc(val): 97.29%\r\n",
      "Epoch: [005/30]   Time: 7.265s   Loss: 3.410e-04   Acc: 85.58%   Loss(val): 9.81e-04   Acc(val): 97.32%\r\n",
      "Epoch: [006/30]   Time: 7.256s   Loss: 3.228e-04   Acc: 85.74%   Loss(val): 9.42e-04   Acc(val): 97.36%\r\n",
      "Epoch: [007/30]   Time: 7.274s   Loss: 3.107e-04   Acc: 85.93%   Loss(val): 8.97e-04   Acc(val): 97.42%\r\n",
      "Epoch: [008/30]   Time: 7.280s   Loss: 2.993e-04   Acc: 86.07%   Loss(val): 8.62e-04   Acc(val): 97.47%\r\n",
      "Epoch: [009/30]   Time: 7.186s   Loss: 2.856e-04   Acc: 86.20%   Loss(val): 7.83e-04   Acc(val): 97.59%\r\n",
      "Epoch: [010/30]   Time: 7.307s   Loss: 2.751e-04   Acc: 86.23%   Loss(val): 7.41e-04   Acc(val): 97.62%\r\n",
      "Epoch: [011/30]   Time: 7.254s   Loss: 2.673e-04   Acc: 86.27%   Loss(val): 7.28e-04   Acc(val): 97.58%\r\n",
      "Epoch: [012/30]   Time: 7.266s   Loss: 2.605e-04   Acc: 86.36%   Loss(val): 7.32e-04   Acc(val): 97.51%\r\n",
      "Epoch: [013/30]   Time: 7.263s   Loss: 2.538e-04   Acc: 86.49%   Loss(val): 7.35e-04   Acc(val): 97.47%\r\n",
      "Epoch: [014/30]   Time: 7.280s   Loss: 2.476e-04   Acc: 86.64%   Loss(val): 7.33e-04   Acc(val): 97.46%\r\n",
      "Epoch: [015/30]   Time: 7.279s   Loss: 2.418e-04   Acc: 86.79%   Loss(val): 7.26e-04   Acc(val): 97.46%\r\n",
      "Epoch: [016/30]   Time: 7.192s   Loss: 2.364e-04   Acc: 86.95%   Loss(val): 7.16e-04   Acc(val): 97.47%\r\n",
      "Epoch: [017/30]   Time: 7.297s   Loss: 2.314e-04   Acc: 87.11%   Loss(val): 7.05e-04   Acc(val): 97.50%\r\n",
      "Epoch: [018/30]   Time: 7.251s   Loss: 2.266e-04   Acc: 87.28%   Loss(val): 6.92e-04   Acc(val): 97.52%\r\n",
      "Epoch: [019/30]   Time: 7.276s   Loss: 2.220e-04   Acc: 87.45%   Loss(val): 6.79e-04   Acc(val): 97.55%\r\n",
      "Epoch: [020/30]   Time: 7.258s   Loss: 2.177e-04   Acc: 87.63%   Loss(val): 6.64e-04   Acc(val): 97.58%\r\n",
      "Epoch: [021/30]   Time: 7.268s   Loss: 2.136e-04   Acc: 87.81%   Loss(val): 6.49e-04   Acc(val): 97.62%\r\n",
      "Epoch: [022/30]   Time: 7.279s   Loss: 2.098e-04   Acc: 87.99%   Loss(val): 6.33e-04   Acc(val): 97.65%\r\n",
      "Epoch: [023/30]   Time: 7.196s   Loss: 2.061e-04   Acc: 88.17%   Loss(val): 6.16e-04   Acc(val): 97.69%\r\n",
      "Epoch: [024/30]   Time: 7.309s   Loss: 2.026e-04   Acc: 88.35%   Loss(val): 5.99e-04   Acc(val): 97.73%\r\n",
      "Epoch: [025/30]   Time: 7.248s   Loss: 1.993e-04   Acc: 88.54%   Loss(val): 5.81e-04   Acc(val): 97.78%\r\n",
      "Epoch: [026/30]   Time: 7.271s   Loss: 1.961e-04   Acc: 88.72%   Loss(val): 5.63e-04   Acc(val): 97.82%\r\n",
      "Epoch: [027/30]   Time: 7.257s   Loss: 1.930e-04   Acc: 88.91%   Loss(val): 5.44e-04   Acc(val): 97.87%\r\n",
      "Epoch: [028/30]   Time: 7.283s   Loss: 1.901e-04   Acc: 89.10%   Loss(val): 5.26e-04   Acc(val): 97.91%\r\n",
      "Epoch: [029/30]   Time: 7.278s   Loss: 1.872e-04   Acc: 89.28%   Loss(val): 5.07e-04   Acc(val): 97.96%\r\n",
      "Epoch: [030/30]   Time: 7.186s   Loss: 1.843e-04   Acc: 89.47%   Loss(val): 4.89e-04   Acc(val): 98.01%\r\n",
      "\t  * Test: Loss 0.001\tAcc: 97.870%\r\n",
      "\u001b[32m[I 2021-06-10 11:23:03,610]\u001b[0m Trial 48 finished with value: 0.000561623241082998 and parameters: {'hidden': 1024, 'nlayers': 1}. Best is trial 10 with value: 0.0004906899121124297.\u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\r\n",
      "  \"num_layers={}\".format(dropout, num_layers))\r\n",
      "Model has a total of 4,207,617 trainable parameters.\r\n",
      "Start training...\r\n",
      "Created cache directory optuner/simple/trial49.\r\n",
      "Epoch: [001/30]   Time: 7.251s   Loss: 7.934e-02   Acc: -1.91%   Loss(val): 1.29e-03   Acc(val): 96.95%\r\n",
      "Epoch: [002/30]   Time: 7.244s   Loss: 4.050e-04   Acc: 85.13%   Loss(val): 1.25e-03   Acc(val): 96.82%\r\n",
      "Epoch: [003/30]   Time: 7.248s   Loss: 3.948e-04   Acc: 84.95%   Loss(val): 1.31e-03   Acc(val): 96.62%\r\n",
      "\u001b[32m[I 2021-06-10 11:23:26,417]\u001b[0m Trial 49 pruned. \u001b[0m\r\n",
      "/opt/conda/lib/python3.7/site-packages/optuna/structs.py:18: FutureWarning: `structs` is deprecated. Classes have moved to the following modules. `structs.StudyDirection`->`study.StudyDirection`, `structs.StudySummary`->`study.StudySummary`, `structs.FrozenTrial`->`trial.FrozenTrial`, `structs.TrialState`->`trial.TrialState`, `structs.TrialPruned`->`exceptions.TrialPruned`.\r\n",
      "  warnings.warn(_message, FutureWarning)\r\n",
      "Study statistics: \r\n",
      "  Number of finished trials:  50\r\n",
      "  Number of pruned trials:  41\r\n",
      "  Number of complete trials:  9\r\n",
      "Best trial:\r\n",
      "  Value:  0.0004906899121124297\r\n",
      "  Params: \r\n",
      "    hidden: 1024\r\n",
      "    nlayers: 1\r\n"
     ]
    }
   ],
   "source": [
    "# simple\n",
    "!python \"optuner.py\" 50 './data/INDEX_BTCUSD, 1D updated.csv' --cache './optuner/simple/' --save --cell LSTM --epochs 30 --shuffle --lr 0.0001 --bs 64 --drop 0.5 --seqlen 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "limiting-broad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:23:27.275980Z",
     "iopub.status.busy": "2021-06-10T11:23:27.275211Z",
     "iopub.status.idle": "2021-06-10T11:23:27.277968Z",
     "shell.execute_reply": "2021-06-10T11:23:27.277498Z"
    },
    "papermill": {
     "duration": 0.1341,
     "end_time": "2021-06-10T11:23:27.278093",
     "exception": false,
     "start_time": "2021-06-10T11:23:27.143993",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# attention\n",
    "!python \"optuner.py\" 100 './data/INDEX_BTCUSD, 1D updated.csv' --cell GRU --epochs 30 --shuffle --attn --lr 0.0001 --bs 64 --drop 0.5 --seqlen 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affecting-pricing",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:23:27.536653Z",
     "iopub.status.busy": "2021-06-10T11:23:27.535956Z",
     "iopub.status.idle": "2021-06-10T11:23:27.538568Z",
     "shell.execute_reply": "2021-06-10T11:23:27.538178Z"
    },
    "papermill": {
     "duration": 0.132426,
     "end_time": "2021-06-10T11:23:27.538673",
     "exception": false,
     "start_time": "2021-06-10T11:23:27.406247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bidirection\n",
    "!python \"optuner.py\" 100 './data/INDEX_BTCUSD, 1D updated.csv' --cell GRU --epochs 30 --shuffle --bi --lr 0.0001 --bs 64 --drop 0.5 --seqlen 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "musical-florist",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-10T11:23:27.796457Z",
     "iopub.status.busy": "2021-06-10T11:23:27.795722Z",
     "iopub.status.idle": "2021-06-10T11:23:27.798110Z",
     "shell.execute_reply": "2021-06-10T11:23:27.798480Z"
    },
    "papermill": {
     "duration": 0.132622,
     "end_time": "2021-06-10T11:23:27.798604",
     "exception": false,
     "start_time": "2021-06-10T11:23:27.665982",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# attention + bidirection\n",
    "!python \"optuner.py\" 100 './data/INDEX_BTCUSD, 1D updated.csv' --cell GRU --epochs 30 --shuffle --attn --bi --lr 0.0001 --bs 64 --drop 0.5 --seqlen 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-passage",
   "metadata": {
    "papermill": {
     "duration": 0.128696,
     "end_time": "2021-06-10T11:23:28.059916",
     "exception": false,
     "start_time": "2021-06-10T11:23:27.931220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hyperparameter configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-plane",
   "metadata": {
    "papermill": {
     "duration": 0.225122,
     "end_time": "2021-06-10T11:23:28.500311",
     "exception": false,
     "start_time": "2021-06-10T11:23:28.275189",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LSTM\n",
    "!python \"optuner.py\" 50 './data/INDEX_BTCUSD, 1D updated.csv' --cache './optuner/simple/' --save --cell LSTM --epochs 30 --shuffle --seqlen 100 --hidden 1024 --nlayers 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU\n",
    "!python \"optuner.py\" 50 './data/INDEX_BTCUSD, 1D updated.csv' --cache './optuner/simple/' --save --cell GRU --epochs 30 --shuffle --seqlen 100 --hidden 1024 --nlayers 2"
   ]
  },
  {
   "source": [
    "# Inference (PyTorch)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best\n",
    "!python \"main.py\" './data/INDEX_BTCUSD, 1D updated.csv' --cache './optuner/simple/' --save --cell LSTM --epochs 50 --shuffle --seqlen 90 --hidden 50 --nlayers 5 --lr 0.001 --bs 32 --drop 0.2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3508.570251,
   "end_time": "2021-06-10T11:23:29.988588",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-06-10T10:25:01.418337",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}